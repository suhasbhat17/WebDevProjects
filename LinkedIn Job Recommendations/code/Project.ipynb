{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "import codecs\n",
    "import math\n",
    "import re\n",
    "import operator\n",
    "import os\n",
    "from os import path\n",
    "from nltk.stem.snowball import SnowballStemmer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "  Read files in the directory to a list of strings. \n",
    "  Input: directory name\n",
    "  Output: Text files listed in the directory as String array\n",
    "\"\"\"\n",
    "def read_documents(dirname):\n",
    "        # Get all files from given directory\n",
    "        files = [dirname+'/'+f for f in os.listdir(dirname)]\n",
    "        documents = []\n",
    "        for filename in files:\n",
    "            # Open file\n",
    "            txt = open(filename)\n",
    "            # Read file as text and add it to String array\n",
    "            documents.append(txt.read())\n",
    "        return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "   Convert a string representing one document into a list of words and convert to lowercase.\n",
    "   Input: document - text\n",
    "   Output: List of words\n",
    "\"\"\"\n",
    "def tokenize(document):\n",
    "        return [t.lower() for t in re.findall(r\"\\w+(?:[-']\\w+)*\", document)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "   Given a list of tokens, stem the english words using Snowball Stemmer\n",
    "   Input: List of words\n",
    "   Output: Stem words using nltk SnowballStemmer \n",
    "\"\"\"\n",
    "def stem(tokens):\n",
    "        stemmer = SnowballStemmer(\"english\")\n",
    "        return [stemmer.stem(t) for t in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "    Given documents, return dict mapping terms to document frequency.\n",
    "    Input: Documents\n",
    "    Output: Document frequency dictionaty\n",
    "\"\"\"\n",
    "def count_doc_frequencies(docs):\n",
    "        res = defaultdict(lambda: 0);\n",
    "        for i in range(len(docs)):\n",
    "            doc = list(set(docs[i]))\n",
    "            for j in range(len(doc)):\n",
    "                res[doc[j]] += 1\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Creates an index in which each postings list contains a list of[doc_id, tf-idf weight] pairs. \n",
    "    Input: Documents & Document frequency of each term\n",
    "    Output: tf-idf weight for each term\n",
    "\"\"\"\n",
    "def create_tfidf_index(docs, doc_freqs):\n",
    "        index = defaultdict(list)\n",
    "        total_docs = len(docs)\n",
    "        for i in range(len(docs)):\n",
    "            term_count = dict(Counter(docs[i]))\n",
    "            for term in term_count:\n",
    "                if term_count[term]/len(docs[i])>0:\n",
    "                    index[term].append([i,(1+math.log10(term_count[term]/len(docs[i])))*math.log10(total_docs/doc_freqs[term])])\n",
    "                else:\n",
    "                    index[term].append([i,math.log10(total_docs/doc_freqs[term])])\n",
    "                    \n",
    "        return index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Return a dict mapping doc_id to length.\n",
    "    Input: tf-idf index weight list\n",
    "    Output: Dictionary mapping with doc length\n",
    "\"\"\"\n",
    "def compute_doc_lengths(index):\n",
    "        lengths = defaultdict(lambda: 0)\n",
    "        for i in index:\n",
    "            for term_count in index[i]:\n",
    "                lengths[term_count[0]] += math.pow(term_count[1],2)\n",
    "        for key, value in lengths.items():\n",
    "            lengths[key] = math.sqrt(value)\n",
    "        return lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "    Given query terms, converts to dictionary with frequency\n",
    "    Input: Query terms\n",
    "    Output: Frequency of terms\n",
    "\"\"\"\n",
    "def query_to_vector(query_terms):\n",
    "        return dict(Counter(query_terms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Given query vector, index and doc length, returns a sorted list of doc_id, score pairs, where the score is the \n",
    "    cosine similarity between the query_vector and the document. \n",
    "    Input: query_vector, document index, doc length\n",
    "    Output: doc_id and cosine score\n",
    "\"\"\"\n",
    "def search_by_cosine(query_vector, index, doc_lengths):\n",
    "        scores = defaultdict(lambda: 0)\n",
    "        for query_term, query_weight in query_vector.items():\n",
    "            for doc_id, doc_weight in index[query_term]:\n",
    "                scores[doc_id] += query_weight * doc_weight \n",
    "        for doc_id in scores:\n",
    "            scores[doc_id] /= doc_lengths[doc_id]\n",
    "        return sorted(scores.items(), key=lambda x: x[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "    Return the top 5 job search results \n",
    "    Input: doc_ids\n",
    "\"\"\"\n",
    "def print_job_info(doc_ids):\n",
    "    res = ''\n",
    "    for doc_id, score in doc_ids[:5]:\n",
    "        fileName = \"jobs/job\" + str(doc_id) + \".txt\"\n",
    "        with open(fileName) as resultFile:\n",
    "            job_info = [next(resultFile) for x in xrange(3)]\n",
    "        print job_info\n",
    "\n",
    "\"\"\" \n",
    "    Return the top 5 user search results \n",
    "    Input: doc_ids\n",
    "\"\"\"\n",
    "def print_user_info(doc_ids):\n",
    "    res = ''\n",
    "    for doc_id, score in doc_ids[:5]:\n",
    "        fileName = \"users/user\" + str(doc_id) + \".txt\"\n",
    "        with open(fileName) as resultFile:\n",
    "            user_info = [next(resultFile) for x in xrange(3)]\n",
    "        print user_info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hans Brough\n",
      "Likes to build useful stuff\n",
      "San Francisco Bay Area\n",
      "Hans started out thinking he wanted to be a Landscape Architect. After attending CAL POLY San Luis Obispo and attaining his degree in the mid- 90's he was wooed to the dark side by the promise of the internet.\n",
      "\n",
      "Luckily many of the problem solving skills they teach in design school translate well to planning and implementing web projects. The tool set is a little different...\n",
      "\n",
      "His career to date has been spent learning and putting to practical use these new tool sets while always trying to keep the end user in mind. He now enjoys participating in web projects using a variety of tools. \n",
      "\n",
      "Specialties: Speed, Object Based JavaScript, Design Patterns, Server Side Javascript, DOM Scripting, YUI, jQuery, Mootools, Dust, HandleBars, MV* Frameworks, PHP, JSP, MySQL, DHTML, CSS, HTML, XHTML, XML, XSLT, CVS, Bug Tracking Systems, Grunt, UI testing with Jasmine, XFN, SEO, XSS, Grunt, Hapi, Node\n",
      "Senior Web Developer\n",
      "Brightsky Labs\n",
      "Web Developer\n",
      "On Sabbatical\n",
      "\n",
      "Senior Web Developer\n",
      "LinkedIn\n",
      "Edit\n",
      "Hack\n",
      "Familytales.org\n",
      "See more\n",
      "Edit\n",
      "Senior UI Developer\n",
      "Threadsy\n",
      "\n",
      "Senior UI Developer\n",
      "SimplyHired\n",
      "\n",
      "Craig B. Rader\n",
      "View\n",
      "Edit\n",
      "\n",
      "Web Developer\n",
      "Simply Hired\n",
      "Web Developer, Client Services\n",
      "eMotion Inc\n",
      "\n",
      "Jeff Paul, PMP, CSM ☁\n",
      "View\n",
      "Web Developer\n",
      "Artmachine.com\n",
      "Production Engineer and Client Side Web Developer\n",
      "Eloquent\n",
      "Share the best parts of your GoPro videos.\n",
      "Right in the moment. Right from your iPhone.\n",
      "\n",
      "http://brightsky.co\n",
      "Taking some time to do things like pound nails, play with code, volunteer in the classroom, make dinner and generally hang out with family / friends.\n",
      "\n",
      "More specifically I'm working on an EdTech project that could be described as 'Magic'​ meets 'Settlers' meets U.S. History in an extensible format for Teachers, Players or even Alternate History Buffs. Gaming is Learning.\n",
      "Of note while building the Linkedin 'Groups' product: The 2010 product overhaul, addition of public groups, submission and moderation workflows and the 2013 'Katy' redesign. Late 2013 - Began migrating the product to a different platform (Play with Dust templating).\n",
      "\n",
      "Assisting in the 2012 launch of the 'Influencers' product.\n",
      "LinkedIn Groups page w/new \"Katy\" design\n",
      "A now defunct project with the goal to provide free access to a unified, well organized, searchable archive of historic, primary documents and social relationships.\n",
      "\n",
      "Designed and implemented methodology that automates the addition of XFN and other semantic markup to primary documents.\n",
      "\n",
      "Designed and implemented a system to determine and quantify strength of relationships among writers of historic letters based in part on harvesting XFN data.\n",
      "\n",
      "Designed and implemented a searcher class to handle user searches along several axis such as person, recipient, year, city, relationship proximity and others.\n",
      "\n",
      "Ongoing work around displaying data model of historic relationships via multiple views. This includes a javascript/json driven 'social map' using relationship proximity values to determine placement and styling of persons within an egocentric network.\n",
      "From the American Revolution to the Civil War\n",
      "Job-a-matic by SimplyHired\n",
      "\n",
      "['UI Developer\\n', 'Visionary Integration Professionals\\n', 'Johnston, IA\\n']\n",
      "['UI Developer \\n', 'Booker Software\\n', 'Greater New York City Area\\n']\n",
      "['UI Developer\\n', 'Open Systems Technologies\\n', 'Greater New York City Area\\n']\n",
      "['UI Developer\\n', 'TransCore\\n', 'Houston, TX, US\\n']\n",
      "['UI Developer\\n', 'Technosoft Corporation\\n', 'New York City, NY, US\\n']\n"
     ]
    }
   ],
   "source": [
    "documents = read_documents('jobs')\n",
    "stemmed_docs = [tokenize(d) for d in documents]\n",
    "doc_freqs = count_doc_frequencies(stemmed_docs)\n",
    "index = create_tfidf_index(stemmed_docs, doc_freqs)\n",
    "doc_lengths = compute_doc_lengths(index)\n",
    "user_documents = read_documents('users')\n",
    "print user_documents[0]\n",
    "user_stemmed_docs = tokenize(user_documents[0])\n",
    "user_query_vector = query_to_vector(user_stemmed_docs)\n",
    "search_job_results = search_by_cosine(user_query_vector,index,doc_lengths)\n",
    "print_job_info(search_job_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
